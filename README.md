# Ganitâ€‘V1: An Adaptive Model for Reasoning Chain Compression and Decompression

## Overview

**Ganitâ€‘V1** is a 14â€¯billionâ€‘parameter language model optimized for multiâ€‘step chainâ€‘ofâ€‘thought reasoning across very long contexts (up to 16,384 tokens). It achieves strong pass@1 and consistency@64 scores on challenging benchmarks, striking a practical balance between answer accuracy and reasoning coherence. This repository provides the trained model weights, evaluation scripts, and result visualizations to help you integrate and assess Ganitâ€‘V1 in your own workflows.

## Datasets

### DatasetÂ 1: RL Compression Data

We begin by sampling multiple responses for each question from the R1â€‘distillâ€‘14B model solely to compute solve rates. Each chain is truncated at 6000 tokensâ€”if a solution requires more than this budget, that sample is marked incorrect. We then calculate the solve rate as the fraction of truncated chains that still arrive at the correct answer. Finally, we filter to those questions whose truncated solve rate lies between 0 and 0.5. This filtered set of problems drives the RL compression phase, guiding Ganitâ€‘V1 to focus on questions that are challenging under a tight token budget.

### DatasetÂ 2: SFT Shortest Chains

The second dataset focuses on **conciseness**.Sampling multiple responses per question from the R1-distill-14B, we identify which responses are correct and then pick the **shortest** chain of thought among them. These shortestâ€‘correct examples form the training data for our supervised fineâ€‘tuning stage, guiding Ganitâ€‘V1 to elaborate its reasoning only as much as necessary when chain length is at a premium.

### DatasetÂ 3: Curriculumâ€‘Learning Data

For our curriculumâ€‘learning set, we start with the R1â€‘distillâ€‘14B checkpoint and annotate each questionâ€™s difficulty using the o3â€‘mini evaluator on the openâ€‘R1 benchmark. We retain only those questions rated at difficultyâ€¯5 or above. Next, we recompute solve rates via multiple samplings from the 14â€¯billionâ€‘parameter model and filter to problems with solve rates between 0.2 and 0.6. By ordering this subset from easier to harder across training epochs, we enable a curriculum learning strategy that gradually exposes Ganitâ€‘V1 to more challenging reasoning tasks.

## Training Strategy

1. **ModelÂ 1 (RL Compression)**  
   - **Base checkpoint**: R1â€‘distillâ€‘14B  
   - **Data**: DatasetÂ 1  
   - **Context length**: 6â€¯000 tokens  
   - **Objective**: Reinforcement learning to compress reasoning chains without losing accuracy.  

   In this phase, Ganitâ€‘V1 learns to identify and retain only the most critical inference steps. By rewarding shorter chains that still produce correct answers under a strict 6â€¯000â€‘token budget, the model internalizes a preference for brevity. This compressed reasoning capability serves as a foundation for more efficient inference in resourceâ€‘constrained settings.

2. **ModelÂ 2 (SFT Decompression)**  
   - **Base checkpoint**: ModelÂ 1  
   - **Data**: DatasetÂ 2  
   - **Objective**: Supervised fineâ€‘tuning to decompress and elaborate reasoning chains, guided by the shortest correct examples.  

   We specifically use the **shortest** correct chains as training examples because they represent the minimal set of inference steps necessary for correctness. By learning from these efficient yet complete chains, Ganitâ€‘V1 acquires the ability to add explanatory detail only where it is neededâ€”avoiding redundant or tangential reasoning while ensuring that every critical step is articulated clearly.

3. **ModelÂ 3 (Curriculum SFT)**  
   - **Base checkpoint**: R1â€‘distillâ€‘14B  
   - **Data**: DatasetÂ 3  
   - **Method**: Curriculum learning SFTâ€”each epoch progresses from easier to harder questions.  

   To bolster robustness on challenging problems, we employ a curriculum learning schedule. Starting with moderateâ€‘difficulty questions and gradually increasing complexity, Ganitâ€‘V1 develops a scaffolded understanding of multiâ€‘step reasoning. This staged approach prevents early overfitting on hard examples and ensures steady performance gains across difficulty levels.


4. **Final Model (Ganitâ€‘V1)**  
   - **Components merged**: ModelÂ 2 (SFT Decompression) and ModelÂ 3 (Curriculum SFT)  
   - **Purpose of merging**: Combining the strengths of both branches creates a versatile model that can generate concise, clear explanations when brevity is important (from ModelÂ 2) while maintaining strong performance on difficult reasoning tasks learned through the curriculum schedule (from ModelÂ 3). The resulting Ganitâ€‘V1 seamlessly adapts its chainâ€‘ofâ€‘thought style to the demands of each question and available context.


## Evaluation

We evaluate Ganitâ€‘V1 using the same metrics and sampling configuration introduced in the DeepSeekâ€‘R1 paper, namely **pass@1** and **cons@64**. However, our evaluation is conducted under a reduced context window of 16,384 tokens, compared to DeepSeekâ€‘R1â€™s 32,768 tokens, to better reflect practical deployment constraints.

- **pass@1**: Measures the fraction of problems correctly solved in the first generated sample.
- **cons@64**: Assesses consistency by sampling 64 reasoning chains per question and computing the majority vote accuracy.

**Evaluation Configuration**:

- Temperature:Â 0.6  
- top_p:Â 0.95  
- Number of sampled chains:Â 64  
- Context window:Â 16,384 tokens  

This setup allows us to benchmark Ganitâ€‘V1â€™s reasoning performance and stability under realistic memory and inference budgets, while maintaining compatibility with the DeepSeekâ€‘R1 evaluation protocol.

We utilize the evaluation framework provided by the [LIMO](https://github.com/GAIR-NLP/LIMO) repository to run inference and compute metrics.

## Results

We evaluate **Ganitâ€‘V1** and several baseline models across four benchmarks: **AIME24**, **AIME25**, **HMMT25**, and **GPQA**. For each, we report `pass@1` and `cons@64`, following the same evaluation configuration.

| Model            | AIME24        |              | AIME25        |              | HMMT25        |              |
|------------------|---------------|--------------|---------------|--------------|---------------|--------------|
|                  | pass@1        | cons@64      | pass@1        | cons@64      | pass@1        | cons@64      |
| LightR1â€‘14B      | 68.8          | 86.67        | 51.15         | 76.67        | 34.11         | 50.00        |
| R1â€‘distillâ€‘14B   | 63.8          | 80.00        | 45.5          | 63.33        | 30.00         | 50.00        |
| R1â€‘distillâ€‘32B   | 66.8          | 83.33        | 49.64         | 73.33        | 33.02         | 53.33        |
| R1â€‘670B          | 75.52         | 86.67        | 61.25         | 83.33        | 42.19         | 56.67        |
|  **Ganitâ€‘V1â€‘14B**ðŸŸ© | **68.13**     | **83.33**    | **51.88**     | **76.67**    | **35.78**     | **56.66**    |
| o1â€‘mini          | 60.05         | 80.00        | 50.71         | 63.33        | 35.15         | 46.67        |
| o3â€‘miniâ€‘low      | 57.44         | 66.67        | 42.6          | 53.33        | 26.61         | 33.33        |
| o3â€‘miniâ€‘medium   | 77.39         | 90.00        | 72.24         | 83.33        | 49.21         | 60.00        |
| o1â€‘preview       | 48.89         | 56.66        | 33.33         | 36.67        | 17.78         | 20.00        |
| gptâ€‘4.5â€‘preview  | 37.78         | 43.33        | 34.44         | 40.00        | 16.67         | 20.00        |
| Model 2          | 66.8          | 83.33        | 50.94         | 73.33        | 33.7          | 40.00        |
| Model 3          | 63.8          | 83.33        | 50.63         | 76.67        | 32.19         | 50.00        |

Notably, we also observe out-of-domain improvement in **GPQA**, indicating that reinforcement learning on mathematics-focused datasets potentially facilitates generalization across diverse domains.

#### âœ… GPQA Benchmark Comparison

| **Model**         | **pass@1** | **cons@64** |
|-------------------|------------|-------------|
| LightR1â€‘14B       | 56.94      | 65.15       |
| R1â€‘distillâ€‘14B    | 54.19      | 64.14       |
| R1â€‘distillâ€‘32B    | 64.57      | 69.70       |
| R1â€‘670B           | 71.88      | 74.24       |
| **Ganitâ€‘V1â€‘14B**ðŸŸ©  | **59.13**  | **66.16**   |
| Model 2           | 56.35      | 66.67       |
| Model 3           | 58.91      | 63.13       |

**Ganitâ€‘V1** demonstrates competitive performance across all datasets, improving over the original distill checkpoints and closely matching or surpassing other strong baselines in several settings. Its consistency across diverse mathematical domains highlights its balanced reasoning ability.


### Response Length Ablation

To assess reasoning efficiency, we compare the **average response lengths** across AIME24, AIME25, and HMMT25. While models like **Light-R1**,  **R1-distillâ€‘14B** and **Model 3** tend to generate longer chains, **Ganitâ€‘V1â€‘14B** consistently produces **more concise responses** without sacrificing performance. This reflects its two-stage training strategyâ€”compressing reasoning via RL and then selectively decompressing only essential steps through SFT.


#### Average Response Length (Tokens)

| Model            | AIME24 | AIME25 | HMMT25 |
|------------------|--------|--------|--------|
| Light-R1         | 10144  | 11330  | 12680  |
| R1-distill-14B   | 9626   | 10878  | 12263  |
| Ganit-V1-14B     | 9532   | 10083  | 12100  |
| Model 2          | 9529   | 10570  | 11950  |
| Model 3          | 10045  | 11236  | 12717  |

