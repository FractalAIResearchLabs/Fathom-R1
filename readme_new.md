# ğŸ§® Ramanujan-Ganit-R1 - Recipe for Unlocking Math Reasoning at o4-mini level with 14B model under 16K context


<div align="center">

[![Hugging Face](https://img.shields.io/badge/HuggingFace-Ramanujan--Ganit--R1--14B--V1-FFD21E?logo=huggingface&style=for-the-badge)](https://huggingface.co/FractalAIResearch/Ramanujan-Ganit-R1-14B-V1)

</div>

<p align="center"> <img src="./images/image.png" style="width: 100%;" id="title-icon">       </p>
---

## ğŸ§  Overview

We introduce **Ramanujan-Ganit-R1-14B-V1** - a compact and compute-efficient 14B language model optimized for **concise and accurate mathematical reasoning**.  
Trained via an extensive pipeline combining **reinforcement learning**, **supervised fine-tuning**, and **curriculum learning**, it delivers **performance rivaling closed-source o4-mini (low)** â€” all while staying within a **16K context window** and a modest compute budget. It achieves an impressive 51.9% Pass@1 accuracy on AIME2025 and 35.8% Pass@1 accuracy on HMMT25(+5.8% and +6.3% improvement over the base model respectively). We have open-sourced our models which we believe will help the community to progress further in this domain. 

---

## ğŸ§ª Why Another Math Model?

Recent advances like DeepSeek-Math and Light-R1 show the potential of large-scale instruction tuning and distillation. However, these approaches often rely on:

- Larger model scales (32B, 70B+)
- Expensive context lengths (32K+)
- Redundant or verbose reasoning

We address these constraints by focusing on **response efficiency** â€” training the model to generate only the **minimal reasoning needed** to reach a correct solution, while scaling curriculum difficulty gradually. 
Here, we unveil the recipe of combining SFT and RL to improve the mathematical reasoning capabilities of a 14B model.


---

## ğŸ—ï¸ Training Pipeline
### Training Recipe for Ramanujan-Ganit-R1-14B-v0.4

In the first training phase, we aim to instill the model with a preference for brevity without compromising correctness. The goal is to train the model to preserve only the most essential parts of the response chain, laying the groundwork for efficient reasoning in resource-constrained settings. We aim to further refine this capability by encouraging the model to elaborate its reasoning only as much as necessary â€” especially for solving more difficult problems. The central intuition is that a model trained on minimal yet complete reasoning chains can learn to be both precise and efficient in its explanations.

We begin by calculating solve rates for each question under a strict 6,000-token constraint by sampling multiple responses from R1-Distill-14B model. Questions whose solve rates fall between 0.0 and 0.5 are retained to form the **RL Compression dataset**. Starting from the R1-Distill-14B checkpoint, we train the model using the **GRPO algorithm**, with a modified objective function to reward shorter chains that reach the correct answer.

Once this reinforcement learning (RL) phase is complete, we build upon the RL checkpoint and continue supervised fine-tuning under a **16K context window** to encourage more detailed reasoning that would be required for solving more complex problems. For this stage, we curate a dataset consisting of hard problems â€” specifically, questions with solve rates between 0.1 and 0.4. Sampling multiple responses from the R1-Distill-14B model, we identify all correct response chains. This time, instead of imposing any token limit, for each question, we select the **shortest correct chain**, forming the **SFT Shortest Chains dataset**.

Through supervised fine-tuning on this dataset, the model learns to explain its reasoning in a more precise and efficient manner â€” elaborating only when necessary and avoiding redundant or tangential steps. The resulting model is named **Ramanujan-Ganit-R1-14B-v0.4**, optimized for concise yet accurate mathematical reasoning.


### Training Recipe for Ramanujan-Ganit-R1-14B-v0.6

This training stage focuses on improving the modelâ€™s performance on **hard mathematical problems** through a curriculum learning strategy. Curriculum learning is a well-established technique for training LLMs, where the model is progressively exposed to more difficult tasks. The idea is to build foundational understanding first and gradually scaffold more complex reasoning, thereby enhancing generalization and reducing overfitting. By first training on moderately difficult examples, the model internalizes essential reasoning patterns, enabling it to tackle harder problems with greater confidence and robustness.

We begin by annotating each questionâ€™s difficulty on a scale from **1 to 10** using **OpenAI's o3mini** model. We retain only those questions rated **5 or above**, and further filter them to include only those having **solve rates between 0.2 and 0.6**, as computed from multiple samples generated by the R1-Distill-14B model. This yields the **Curriculum Learning dataset**.

The **R1-Distill-14B** checkpoint is chosen as the base model for **supervised fine-tuning (SFT)** under a **16K context window**. A **curriculum schedule** is employed across each training epoch, such that questions are presented in order of increasing difficulty. This structured progression â€” from easier to harder examples â€” allows the model to incrementally develop and enhance its problem-solving skills.

The resulting model is **Ramanujan-Ganit-R1-14B-v0.6**, a model that demonstrates improved reasoning performance on challenging mathematical problems.


## Model Merging
The final model, **Ramanujan-Ganit-R1-14B-V1**, is obtained by **merging** the 2 individual reasoning models from the aforementioned training stages:

- **Ramanujan-Ganit-R1-14B-V0.4 (RL + SFT)** contributes the ability to generate efficient, well-articulated reasoning chains that are as brief as they are correct.
- **Ramanujan-Ganit-R1-14B-V0.6 (Curriculum SFT)** enhances resilience on difficult problems and promotes stepwise learning across complexity levels.

By combining these complementary strengths of the 2 models, **Ramanujan-Ganit-R1-14B-V1** maximizes its potential of solving highly complicated math problems accurately, while simultaneously offering a **concise explanation** for the same.



## ğŸ“Š Evaluation
We evaluate Ramanujan-Ganitâ€‘R1-14B-V1 using the same metrics and sampling configuration introduced in the DeepSeekâ€‘R1 paper, namely **pass@1** and **cons@64**. However, our evaluation is conducted under a reduced context window of 16,384 tokens, compared to DeepSeekâ€‘R1â€™s 32,768 tokens, to better reflect practical deployment constraints.

- **pass@1**: Measures the fraction of problems correctly solved in the first generated sample.
- **cons@64**: Assesses consistency by sampling 64 reasoning chains per question and computing the majority vote accuracy.

**Evaluation Configuration**:

- Temperature:Â 0.6  
- top_p:Â 0.95  
- Number of sampled chains:Â 64  
- Context window:Â 16,384 tokens  

This setup allows us to benchmark Ramanujan-Ganit-R1-14Bâ€‘V1â€™s reasoning performance and stability under realistic memory and inference budgets, while maintaining compatibility with the DeepSeekâ€‘R1 evaluation protocol.

We utilize the evaluation framework provided by the [LIMO](https://github.com/GAIR-NLP/LIMO) repository to run inference and compute metrics.
For detailed instructions and implementation details, please refer to [`eval/README.md`](./eval/readme.md).


## Results
We evaluate and compare **Ramanujan-Ganitâ€‘R1-14B-V1** with several baseline models across 3 challenging benchmarks:  **AIME25**, **HMMT25**, and **GPQA**. For each, we report `pass@1` and `cons@64`, following the same evaluation configuration.

| Model            | AIME25         |               | HMMT25         |               |
|------------------|----------------|---------------|----------------|---------------|
|                  | pass@1         | cons@64       | pass@1         | cons@64       |
| **Closed-Source Models**               |                |               |                |               |
| o1â€‘mini          | 50.71          | 63.33         | 35.15          | 46.67         |
| o3â€‘miniâ€‘low      | 42.6           | 53.33         | 26.61          | 33.33         |
| o3â€‘miniâ€‘medium   | 72.24          | 83.33         | 49.21          | 60.00         |
| o4-mini-low      | 60.2           | 76.67         | 39.11          | 53.33         |
| o1â€‘preview       | 33.33          | 36.67         | 17.78          | 20.00         |
| gptâ€‘4.5â€‘preview  | 34.44          | 40.00         | 16.67          | 20.00         |
| **Open-Source Models**               |                |               |                |               |
| LightR1â€‘14B      | 51.15          | 76.67         | 34.11          | 50.00         |
| R1â€‘distillâ€‘14B   | 45.5           | 63.33         | 30.00          | 50.00         |
| R1â€‘distillâ€‘32B   | 49.64          | 73.33         | 33.02          | 53.33         |
| R1â€‘670B          | 61.25          | 83.33         | 42.19          | 56.67         |
| Ramanujan-Ganitâ€‘R1-14B-V0.4         | 50.94          | 73.33         | 33.7           | 40.00         |
| Ramanujan-Ganitâ€‘R1-14B-V0.6          | 50.63          | 76.67         | 32.19          | 50.00         |
| **Ramanujan-Ganitâ€‘R1-14B-V1** | **51.88**      | **76.67**     | **35.78**      | **56.66**     |

**Ramanujan-Ganitâ€‘R1-14B-V1** demonstrates highly competitive performance across all datasets, improving over the original R1-distilled models while closely matching or surpassing other strong baselines in several settings. 
On both AIME 25 and HMMT 25, our model shows the highest pass@1 as well as cons@64 scores among all the open-source models (including the bigger R1-Distilled-32B model), with R1-670B being the only exception.

In fact, we observe that Ramanujan-Ganit-R1-14B-V1 is superior to even some of the OpenAI reasoning models, including **o1-mini** and **o3-mini (low)** and it's performance closely matches that of newly released **o4-mini (low)**.
Its consistency across diverse mathematical domains highlights its balanced reasoning ability.

---

## ğŸŒ Generalization Beyond Math

Notably, we also observe out-of-domain improvement in **GPQA**, even though there wasn't a single instance of science reasoning based questions in our training data. 
This indicates that training solely on mathematics-focused datasets potentially facilitates generalization across diverse domains, a finding similar to what Light-R1 had observed.
#### âœ… GPQA Benchmark Comparison
| **Model**         | **pass@1** | **cons@64** |
|-------------------|------------|-------------|
| LightR1â€‘14B       | 56.94      | 65.15       |
| R1â€‘distillâ€‘14B    | 54.19      | 64.14       |
| R1â€‘distillâ€‘32B    | 64.57      | 69.70       |
| R1â€‘670B           | 71.88      | 74.24       |
| Ramanujan-Ganitâ€‘R1-14B-V0.4           | 56.35      | 66.67       |
| Ramanujan-Ganitâ€‘R1-14B-V0.6           | 58.91      | 63.13       |
| **Ramanujan-Ganitâ€‘R1-14B-V1**  | 59.13 | 66.16  |


## âœ‚ï¸ Ablation Study on Token Efficiency
To assess reasoning efficiency, we compare the **average response lengths** across  AIME25, and HMMT25. While models like **Light-R1-14B**,  **R1-distillâ€‘14B** and **Ramanujan-Ganitâ€‘R1-14B-V0.6** tend to generate longer chains, **Ramanujan-Ganitâ€‘R1-14B-V1** consistently produces **more concise responses** without sacrificing performance. 
#### Average Response Length (Tokens)

| Model            | AIME25 | HMMT25 |
|------------------|--------|--------|
| Light-R1-14B         | 11330  | 12680  |
| R1-distill-14B   | 10878  | 12263  |
| Ramanujan-Ganitâ€‘R1-14B-V0.4          | 10570  | 11950  |
| Ramanujan-Ganitâ€‘R1-14B-V0.6         | 11236  | 12717  |
| **Ramanujan-Ganitâ€‘R1-14B-V1**      | 10083  | 12100  |


## ğŸ“œ License

ğŸŸ¡ Released under **Apache 2.0** â€” free for commercial and research use.

---

## ğŸ“– Citation

```bibtex
@misc{ramanujan14b2024,
  title={Ramanujan-Ganit-R1-14B},
  author={Fractal AI Research},
  year={2024},
  url={https://huggingface.co/FractalAIResearch/Ramanujan-Ganit-R1-14B-V1}
}
